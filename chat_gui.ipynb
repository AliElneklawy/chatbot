{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import json\n",
    "from string import punctuation\n",
    "import pickle\n",
    "import random\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "intents = json.loads(open('intents.json').read())\n",
    "model = load_model('chatbot_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_sentence(sentence: str) -> list:\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    lemmatized = [lemmatizer.lemmatize(word.lower()) for word in tokenized]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(words: list, sentence_words: list) -> np.array:\n",
    "    bag = [0] * len(words)\n",
    "    for sentence_word in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if sentence_word == word:\n",
    "                bag[i] = 1\n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(msg: np.array) -> list:\n",
    "    msg_class_probs = model.predict(np.array([msg]))[0]\n",
    "    results = [[i,r] for i, r in enumerate(msg_class_probs)]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(msg_class: str) -> str:\n",
    "    for tag in intents['intents']:\n",
    "        if tag['tag'] == msg_class:\n",
    "            random_response = random.choice(tag['responses'])\n",
    "    return random_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(msg: str):\n",
    "    msg = cleanup_sentence(msg)\n",
    "    msg = bag_of_words(words, msg)\n",
    "    msg_class_probs = predict_class(msg)\n",
    "    msg_class = classes[msg_class_probs[0][0]]\n",
    "    response = get_response(msg_class)\n",
    "    return response, msg_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "The world tongue-twister champion just got arrested. I hear they're gonna give him a really tough sentence.\n"
     ]
    }
   ],
   "source": [
    "msg = 'I feel sad. tell me a joke'\n",
    "response, msg_class = main(msg)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
